package maxdeep;

import com.maxeler.maxcompiler.v2.kernelcompiler.Kernel;
import com.maxeler.maxcompiler.v2.kernelcompiler.KernelParameters;
import com.maxeler.maxcompiler.v2.kernelcompiler.types.base.DFEVar;
import com.maxeler.maxcompiler.v2.kernelcompiler.types.base.DFEType;
import com.maxeler.maxcompiler.v2.kernelcompiler.types.composite.DFEVector;
import com.maxeler.maxcompiler.v2.kernelcompiler.types.composite.DFEVectorType;
import com.maxeler.maxcompiler.v2.kernelcompiler.stdlib.memory.Memory;
import com.maxeler.maxcompiler.v2.kernelcompiler.stdlib.core.CounterChain;
import com.maxeler.maxcompiler.v2.kernelcompiler.stdlib.KernelMath;
import com.maxeler.maxcompiler.v2.utils.MathUtils;

import java.util.*;

/**
 * The post processing kernel is aiming at accumulate the result from
 * the previous kernels like MPC, and do some post processing NORM or POOL
 * or ReLU.
 * 
 * NOTE: Currently I think it would be a good idea to implement POOL as a
 * KernelLib or not. Will consider this later.
 * (2017-03-04) Ruizhe Zhao
 */
public class ConvolutionLayerPostProcessKernel extends Kernel {

  public static final String INP_NAME = "POST_PROCESS_INP";
  public static final String OUT_NAME = "POST_PROCESS_OUT";

  public static final String SCALAR_HEIGHT_INP_NAME
    = "CONV_POST_SCALAR_HEIGHT_INP";
  public static final String SCALAR_WIDTH_INP_NAME
    = "CONV_POST_SCALAR_WIDTH_INP";
  public static final String SCALAR_NUM_CHANNELS_INP_NAME
    = "CONV_POST_SCALAR_NUM_CHANNELS_INP";
  public static final String SCALAR_NUM_FILTERS_INP_NAME
    = "CONV_POST_SCALAR_NUM_FILTERS_INP";
  public static final String SCALAR_KERNEL_SIZE_INP_NAME
    = "CONV_POST_SCALAR_KERNEL_SIZE_INP";

  /**
   * @param params The default kernel parameters made by Manager
   * @param maxHeight Maximum height of the input feature map
   * @param maxWidth Maximum width of the input feature map
   * @param maxNumChannels Maximum number of channels of the input feature map
   * @param maxNumFilters Maximum number of filters of the input feature map
   * @param maxKernelSize Maximum shape of the kernel
   * @param hasPooling Whether this layer has a pooling layer appended
   * @param dbg The flag to use DEBUG mode
   */
  public ConvolutionLayerPostProcessKernel(
    KernelParameters params,
    int numPipes,
    int maxHeight,
    int maxWidth,
    int maxNumChannels,
    int maxNumFilters,
    int maxKernelSize,
    boolean dbg 
  ) {
    super(params);

    optimization.pushPipeliningFactor(1.0);
    optimization.pushDSPFactor(1);

    // configured by set-and-test
    int latency = 5;

    DFEType type = dfeUInt(32);
    DFEType scalarParamType = dfeUInt(32);

    /**
     * The scalar IO of this kernel is similar to the cache kernel
     * Scalar IOs:
     * - height: The height of the current input feature map
     * - width: The width of the current input feature map
     * - numChannels: The number of channels of the current input feature map
     * - numFilters: The number of filters of the current output feature map
     * - kernelSize: The kernel size of the current computation
     */
    DFEVar height      = io.scalarInput(SCALAR_HEIGHT_INP_NAME, scalarParamType);
    DFEVar width       = io.scalarInput(SCALAR_WIDTH_INP_NAME, scalarParamType);
    DFEVar numChannels = io.scalarInput(SCALAR_NUM_CHANNELS_INP_NAME, scalarParamType);
    DFEVar numFilters  = io.scalarInput(SCALAR_NUM_FILTERS_INP_NAME, scalarParamType);
    DFEVar kernelSize  = io.scalarInput(SCALAR_KERNEL_SIZE_INP_NAME, scalarParamType);

    DFEVar paddedNumFilters
      = KernelMath.ceil(
          numFilters.cast(dfeFloat(8, 24)) / numPipes
        ).cast(scalarParamType)
        * numPipes;

    /**
     * Other constants.
     * - outputHeight: the height of the output feature map
     * - outputWidth: the width of the output feature map
     */
    DFEVar outputHeight = height - kernelSize + 1;
    DFEVar outputWidth = width - kernelSize + 1;

    /**
     * Counters
     *
     */
    CounterChain chain = control.count.makeCounterChain();
    DFEVar c = chain.addCounter(numChannels, 1).cast(dfeUInt(32));
    DFEVar f = chain.addCounter(paddedNumFilters, numPipes).cast(dfeUInt(32));
    // DFEVar h = chain.addCounter(outputHeight, 1).cast(dfeUInt(32));
    // DFEVar w = chain.addCounter(outputWidth, 1).cast(dfeUInt(32));
    DFEVar x = chain.addCounter(outputHeight * outputWidth, 1).cast(dfeUInt(32));
    DFEVar t = control.count.simpleCounter(32).cast(dfeUInt(32));

    /**
     * Output cache:
     * This cache stores the temporary result of each filter. So it's 
     * size is maxHeight * maxWidth * maxNumFilters, which is definitely
     * larger than the space requirement of this cache.
     */
    int cacheSize         = maxHeight * maxWidth * maxNumFilters / numPipes;
    int cacheTotalSize    = maxHeight * maxWidth * maxNumFilters;
    int cacheAddrBitWidth = MathUtils.bitsToAddress(cacheSize);
    DFEType cacheAddrType = dfeUInt(cacheAddrBitWidth);
    
    List<Memory<DFEVar>> cache = new ArrayList<Memory<DFEVar>>();
    for (int pipe = 0; pipe < numPipes; pipe ++) 
      cache.add(mem.alloc(type, cacheSize));

    /**
     * Input value
     */
    int inpVecSize = numPipes;
    DFEVectorType<DFEVar> inpVecType
      = new DFEVectorType<DFEVar>(type, inpVecSize);
    DFEVector<DFEVar> inpVec = io.input(INP_NAME, inpVecType);

    DFEVectorType<DFEVar> outVecType
      = new DFEVectorType<DFEVar>(type, numPipes);
    DFEVector<DFEVar> outVec = outVecType.newInstance(this);

    /**
     * Accumulator
     * This accumulator is quite interesting - It will read the address that
     * belongs to the current cycle, but update the address a few cycles 
     * in the past, which is achieved by using the stream offset.
     * As there is a large gap between the read and update of one address in
     * the output feature map (outputHeight * outputWidth).
     *
     * TODO: Is it possible to decide the latency automatically?
     *
     * TODO: Is it possible to allow multiple update in one cycle without
     * duplicating the BRAM?
     * (2017-03-04) Ruizhe Zhao
     */

    x = optimization.pipeline(x);
    x = optimization.pipeline(x);

    for (int pipe = 0; pipe < numPipes; pipe ++) {
      DFEVar cacheAddr        = (optimization.pipeline(x) + (f / numPipes) * outputHeight * outputWidth).cast(cacheAddrType);
      DFEVar sum              = (c === 0) ? constant.var(0) : cache[pipe].read(cacheAddr);
      sum                     = optimization.pipeline(sum);
      DFEVar newSum           = sum + inpVec[pipe];
      DFEVar cacheWriteAddr   = stream.offset(cacheAddr, -latency);
      DFEVar cacheWriteValue  = stream.offset(newSum, -latency);
      DFEVar cacheWriteEnable = t >= latency;

      cache[pipe].write(cacheWriteAddr, cacheWriteValue, cacheWriteEnable);

      outVec[pipe] <== newSum;
    }

    /**
     * NOTE: According to today's timing report, there is a delay between the minus 
     * and the comparison.
     * Just add this redundant code here, may remove it later.
     * Ref: 160MHz
     * (2017-03-10) Ruizhe Zhao
     * 
     * NOTE: Switched the place of the pipelining and the casting
     * (2017-03-12) Ruizhe Zhao
     */
    DFEVar numChannelsMinusOne = numChannels - 1;
    numChannelsMinusOne = numChannelsMinusOne.cast(dfeUInt(32));
    numChannelsMinusOne = optimization.pipeline(numChannelsMinusOne);

    /**
     * NOTE: According to the diagram and the timing report, it seems that if
     * we delayed the minus one, we might also need to delay the counter.
     * (2017-03-10) Ruizhe Zhao
     */
    DFEVar cPipelinedLevelOne = optimization.pipeline(c); 

    io.output(OUT_NAME, outVec, outVecType, cPipelinedLevelOne === numChannelsMinusOne);

    if (dbg) {
      debug.simPrintf("CONV POST PROCESS: %d - c %d out %KObj%\n", t, c, outVec);
    }
  }
}
