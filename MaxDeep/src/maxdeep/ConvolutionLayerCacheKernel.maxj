package maxdeep;

import com.maxeler.maxcompiler.v2.kernelcompiler.Kernel;
import com.maxeler.maxcompiler.v2.kernelcompiler.KernelLib;
import com.maxeler.maxcompiler.v2.kernelcompiler.KernelParameters;
import com.maxeler.maxcompiler.v2.kernelcompiler.types.base.DFEVar;
import com.maxeler.maxcompiler.v2.kernelcompiler.types.base.DFEType;
// import com.maxeler.maxcompiler.v2.kernelcompiler.types.base.DFEFix;
import com.maxeler.maxcompiler.v2.kernelcompiler.types.composite.DFEVector;
import com.maxeler.maxcompiler.v2.kernelcompiler.types.composite.DFEVectorType;
import com.maxeler.maxcompiler.v2.kernelcompiler.stdlib.core.Mem;
import com.maxeler.maxcompiler.v2.kernelcompiler.stdlib.memory.Memory;
import com.maxeler.maxcompiler.v2.kernelcompiler.stdlib.core.CounterChain;
import com.maxeler.maxcompiler.v2.kernelcompiler.stdlib.KernelMath;
import com.maxeler.maxcompiler.v2.utils.MathUtils;

public class ConvolutionLayerCacheKernel extends Kernel {

  public static final String INP_NAME     = "CONV_CACHE_INP";
  public static final String WGT_NAME     = "CONV_CACHE_WGT";
  public static final String OUT_INP_NAME = "CONV_CACHE_OUT_INP";
  public static final String OUT_WGT_NAME = "CONV_CACHE_OUT_WGT";

  public static final String SCALAR_HEIGHT_INP_NAME
    = "CONV_CACHE_SCALAR_HEIGHT_INP";
  public static final String SCALAR_WIDTH_INP_NAME
    = "CONV_CACHE_SCALAR_WIDTH_INP";
  public static final String SCALAR_NUM_CHANNELS_INP_NAME
    = "CONV_CACHE_SCALAR_NUM_CHANNELS_INP";
  public static final String SCALAR_NUM_FILTERS_INP_NAME
    = "CONV_CACHE_SCALAR_NUM_FILTERS_INP";
  public static final String SCALAR_KERNEL_SIZE_INP_NAME
    = "CONV_CACHE_SCALAR_KERNEL_SIZE_INP";

  public static final DFEType DEFAULT_TYPE = dfeUInt(32);
  public int bitOfInteger = 8;
  public int bitOfFraction = 24;

  /**
   * Design principles:
   * This is the kernel for caching the input data for the convolution
   * layer. There are mainly two kinds of buffers: input buffer and 
   * weights buffer. The input buffer will be organised as line buffers,
   * and the weights buffer is simply a DFEVector.
   * 
   * Parameters:
   * @param params The default kernel parameters made by Manager
   * @param maxHeight Maximum height of the input feature map
   * @param maxWidth Maximum width of the input feature map
   * @param maxNumChannels Maximum number of channels of the input feature map
   * @param maxNumFilters Maximum number of filters of the input feature map
   * @param maxKernelSize Maximum shape of the kernel
   * @param dbg The flag to use DEBUG mode
   */
  public ConvolutionLayerCacheKernel(
    KernelParameters params,
    int numPipes,
    int maxHeight,
    int maxWidth,
    int maxNumChannels,
    int maxNumFilters,
    int maxKernelSize,
    boolean dbg 
  ) {
    super(params);

    DFEType type = dfeUInt(32);
    DFEType scalarParamType = dfeUInt(32);

    optimization.pushPipeliningFactor(1.0);
    optimization.pushDSPFactor(1);

    /**
     * Scalar IOs:
     * - height: The height of the current input feature map
     * - width: The width of the current input feature map
     * - numChannels: The number of channels of the current input feature map
     * - numFilters: The number of filters of the current output feature map
     * - kernelSize: The kernel size of the current computation
     */
    DFEVar height      = io.scalarInput(SCALAR_HEIGHT_INP_NAME, scalarParamType);
    DFEVar width       = io.scalarInput(SCALAR_WIDTH_INP_NAME, scalarParamType);
    // DFEVar numChannels = io.scalarInput(SCALAR_NUM_CHANNELS_INP_NAME, scalarParamType);
    DFEVar numFilters  = io.scalarInput(SCALAR_NUM_FILTERS_INP_NAME, scalarParamType);
    DFEVar kernelSize  = io.scalarInput(SCALAR_KERNEL_SIZE_INP_NAME, scalarParamType);

    /**
     * Counters:
     *
     * This part of code declares the counters and the counter chain.
     * It follows the channel-major style, and ignores the padding and
     * stride currently.
     * There are several signals, especially for valid and enable, 
     * depends on these counter DFEVars.
     *
     * (2017-03-03) Ruizhe Zhao
     */
    CounterChain chain = control.count.makeCounterChain();
    // DFEVar c = chain.addCounter(numChannels, 1).cast(dfeInt(32));
    DFEVar f = chain.addCounter(numFilters, numPipes).cast(dfeUInt(32));
    DFEVar h = chain.addCounter(height, 1).cast(dfeUInt(32));
    DFEVar w = chain.addCounter(width, 1).cast(dfeUInt(32));
    
    /**
     * Input:
     *
     * These lines creates the input stream of the feature map. Only in the
     * first filter's computation of each channel will the input be enabled,
     * which is controlled by inpEnable.
     * Unlike the wgtVec, which takes (K * K) variables in each cycle,
     * the inp DFEVar will only take one element in each cycle.
     * Following logics for this variable will be implemented in the InputBuffer
     * kernel.
     *
     * (2017-03-03) Ruizhe Zhao
     */
    DFEVar inpEnable = (f === 0);
    DFEVar inp = io.input(INP_NAME, DEFAULT_TYPE, inpEnable).cast(type);

    /**
     * Weights:
     *
     * This snippets creates the wgtVec vector, which has shape (K * K).
     * This wgtVec is completely fed by the io.input(), and the input
     * action will only be enabled once for each filter and each channel,
     * controlled by wgtEnable.
     * 
     * (2017-03-03) Ruizhe Zhao
     * 
     * TODO: The input stream might be revised to take only one element at 
     * one tick. It is worthless to take all of them in when the input feature
     * map is not ready.
     * One thing is also restricting this variable: The scalar input kernelSize
     * should have the same value as maxKernelSize.
     * This should not be the case for streams between cache and compute kernels,
     * but we might need some padding/unpadding work for the input stream from LMem.
     * (2017-03-03) Ruizhe Zhao
     * 
     * The idea of the NEW wgtEnable is to read K * K elements in K * K cycles,
     * only when the current counter shows it is the first K * K cycles in the
     * frame.
     * The weights will be saved statically in a BRAM buffer, which might be duplicated
     * for multiple read operations in the same cycle.
     */
    int wgtVecSize = maxKernelSize * maxKernelSize;
    int wgtVecTotalSize = wgtVecSize * numPipes;

    DFEVar wgtEnable                 = (h * width + w).cast(scalarParamType) < (kernelSize * kernelSize * numPipes);
    DFEVar wgtValid                  = ~wgtEnable;
    DFEVar wgt                       = io.input(WGT_NAME, DEFAULT_TYPE, wgtEnable).cast(type);
    DFEVectorType<DFEVar> wgtVecType = new DFEVectorType<DFEVar>(type, wgtVecTotalSize);
    DFEVector<DFEVar> wgtVec         = wgtVecType.newInstance(this);
    Memory<DFEVar> wgtMem            = mem.alloc(type, wgtVecTotalSize);
    DFEType wgtAddrType              = dfeUInt(MathUtils.bitsToAddress(wgtVecTotalSize));
    DFEVar wgtAddr                   = wgtAddrType.newInstance(this);

    DFEVar kernelArea = kernelSize * kernelSize;
    DFEVar wgtReadIdx = h * width + w;
    
    KernelMath.DivModResult m = KernelMath.divMod(wgtReadIdx, kernelArea);
    DFEVar wgtReadPipeIdx = m.getQuotient().cast(dfeUInt(32));
    DFEVar wgtReadInPipeIdx = m.getRemainder().cast(dfeUInt(32));
    wgtAddr <== (wgtReadPipeIdx * wgtVecSize + wgtReadInPipeIdx).cast(wgtAddrType);

    wgtMem.write(wgtAddr, wgt, wgtEnable);

    for (int pipe = 0; pipe < numPipes; pipe ++) {
      for (int idx = 0; idx < wgtVecSize; idx ++) {
        DFEVar i = constant.var(idx).cast(dfeUInt(32));
        DFEVar p = constant.var(pipe).cast(dfeUInt(32));
        DFEVar readAddr = (i + p * wgtVecSize).cast(wgtAddrType);

        kernelArea = optimization.pipeline(kernelArea);

        wgtVec[idx + pipe * wgtVecSize] <==
          (i < kernelArea)
          ? wgtMem.read(readAddr)
          : constant.var(0);
      }
    }
    
    /**
     * Output:
     *
     * This is the output vector for the input feature map, which is generated
     * by the InputBuffer.
     *
     * TODO: May decide what the length of the outputVec later.
     * (2017-03-03) Ruizhe Zhao
     */
    int outVecSize = maxKernelSize * maxKernelSize;
    int outVecTotalSize = outVecSize * numPipes;
    DFEVectorType<DFEVar> outVecType = 
      new DFEVectorType<DFEVar>(type, outVecSize);
    DFEVector<DFEVar> out = outVecType.newInstance(this);
    DFEVar outValid = dfeBool().newInstance(this);

    DFEVectorType<DFEVar> outVecTotalType = 
      new DFEVectorType<DFEVar>(type, outVecTotalSize);
    DFEVector<DFEVar> outVec = outVecTotalType.newInstance(this);

    for (int pipe = 0; pipe < numPipes; pipe ++)
      for (int idx = 0; idx < outVecSize; idx ++)
        outVec[pipe * outVecSize + idx] <== optimization.pipeline(out[idx]);

    /**
     * Input Buffer:
     * This is merely an instantiation of the InputBufferKernel.
     *
     * TODO: The always high signal for this buffer looks a bit weird,
     * may revise it later.
     * (2017-03-03) Ruizhe Zhao
     * Revised it to use c, in order to avoid c's warnings
     * (2017-03-03) Ruizhe Zhao
     */
    new InputBufferKernel(
      this,
      type,
      inp,
      out,
      h,
      w,
      inpEnable,
      outValid,
      height,
      width,
      kernelSize,
      maxHeight,
      maxWidth,
      maxKernelSize,
      dbg
    );

    /**
     * There are two output streams of this kernel, one is for K * K elements from 
     * the input feature map, the other one is for K * K weights that remain the 
     * same for the computation in the same channel.
     *
     * As these two streams will be triggered at the same tick, we add a 
     * io.forceOutputsTogether to let the compiler know about this issue.
     * See the Debugging and Optimization Tutorial (p30).
     * Currently I don't know how this will affect the optimisation in the compiler,
     * hope it will help.
     *
     * TODO: Make this two output stream aware of the following parallelisation
     * (2017-03-03) Ruizhe Zhao
     */
    io.output(OUT_INP_NAME, outVec, outVecTotalType, outValid);
    io.output(OUT_WGT_NAME, wgtVec, wgtVecType, wgtValid);
    // io.forceOutputsTogether(OUT_INP_NAME, OUT_WGT_NAME);

    if (dbg) {
      DFEVar tick = control.count.simpleCounter(32);
      debug.simPrintf("%d: counters - (%3d, %3d, %3d)\n",
        tick, f, h, w);
      debug.simPrintf("%d: inpVec   - %KObj%\n", tick, inp);
      debug.simPrintf("%d: wgtVec   - %KObj%\n", tick, wgtVec);
      debug.simPrintf("%d: outVec   - %KObj%\n", tick, outVec);
      debug.simPrintf("%d: outValid - %KObj%\n", tick, outValid);
      debug.simPrintf("%d: wgtValid - %KObj%\n", tick, wgtValid);
      debug.simPrintf("%d: valid    - %KObj%\n", tick, outValid & wgtValid);
    }
  }
}

class InputBufferKernel extends KernelLib {

  /**
   * This kernel is the buffer of the input feature map of the convolution layer.
   *
   * Unlike those RTL based approach, this kernel will fully utilise the 
   * functionalities of streams in MaxJ. 
   *
   * @param owner The owner kernel of this KernelLib
   * @param type The type of the data inside the streams
   * @param inp The input stream to the input buffer
   * @param out The output stream to the computing kernel
   * @param enable The signal to enable this buffer to run
   * @param writeBRAMEnable Whether or not the inner BRAM should be overwritten
   * @param valid The valid signal of the output vector
   * @param height The height of the input feature map
   * @param width The width of the input feature map
   * @param kernelSize The size of the kernel
   * @param maxKernelSize The size of the kernel
   * @param maxHeight The height of the input feature map
   * @param maxWidth The width of the input feature map
   * @param dbg The debug flag
   */
  public InputBufferKernel(
    KernelLib owner,
    DFEType type,
    DFEVar inp,
    DFEVector<DFEVar> out,
    DFEVar h,
    DFEVar w,
    DFEVar writeBRAMEnable,
    DFEVar valid,
    DFEVar height,
    DFEVar width,
    DFEVar kernelSize,
    int maxHeight,
    int maxWidth,
    int maxKernelSize,
    boolean dbg
  ) {
    super(owner);

    /**
     * This counter chain has the same value as the owner (Cache kernel).
     * TODO: Might need to think it over and share these variables?
     * (2017-03-03) Ruizhe Zhao
     */
    // CounterChain chain = owner.control.count.makeCounterChain(enable);
    // DFEVar h = chain.addCounter(height, 1).cast(dfeUInt(32));
    // DFEVar w = chain.addCounter(width, 1).cast(dfeUInt(32));
    // h = optimization.pipeline(h);
    // w = optimization.pipeline(w);

    /**
     * The address for the read/write of the current input buffer.
     * Also the cache is allocated here.
     * There is only one port needed for this cache BRAM, which will
     * greatly reduce the BRAM usage of this layer (or there will be
     * multiple copies).
     */
    DFEVar addr = h * width + w;
    addr = addr.cast(dfeUInt(MathUtils.bitsToAddress(maxHeight * maxWidth)));
    Memory<DFEVar> cache = mem.alloc(type, maxHeight * maxWidth);
    DFEVar cacheOut = cache.port(
        addr,
        inp,
        writeBRAMEnable,
        Mem.RamWriteMode.WRITE_FIRST);

    /**
     * This valid signal controls when to output the outVec and the 
     * wgtVec. Note that only when the stream can offset a full kernel
     * of the input feature map this valid signal will be high.
     */
    DFEVar kernelSizeMinusOne = kernelSize - 1;
    kernelSizeMinusOne = optimization.pipeline(kernelSizeMinusOne);
    valid <== (h >= kernelSizeMinusOne) & (w >= kernelSizeMinusOne);

    /**
     * This double-loop constructs the output vector. Note that the 
     * sequence of the elements of the output vector are iterated
     * reversely.
     */
    for (int idx = 0; idx < maxKernelSize * maxKernelSize; idx ++) {
      DFEVar i = constant.var(idx).cast(dfeUInt(32));

      KernelMath.DivModResult m = KernelMath.divMod(i, kernelSize);
      DFEVar x = (kernelSize - m.getQuotient() - 1).cast(dfeInt(32));
      DFEVar y = (kernelSize - m.getRemainder() - 1).cast(dfeInt(32));

      // NOTE: Please make sure the min_offset is the min case of the second
      // term.
      // (2017-03-03) Ruizhe Zhao
      DFEVar kernelArea = kernelSize * kernelSize; 
      kernelArea = optimization.pipeline(kernelArea);
      out[idx] <== (i < kernelArea)
        ? stream.offset(
            cacheOut,
            - (x * width.cast(dfeInt(32)) + y),
            - (maxKernelSize * maxWidth + maxKernelSize),
            0)
        : constant.var(0).cast(type);
    }
    
  }

}
