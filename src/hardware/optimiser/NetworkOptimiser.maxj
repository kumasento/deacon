package maxdeep.optimiser;

// import com.maxeler.maxcompiler.v2.build.EngineParameters;
import com.maxeler.maxcompiler.v2.managers.custom.DFELink;
import com.maxeler.maxcompiler.v2.managers.custom.blocks.KernelBlock;
import com.maxeler.maxcompiler.v2.managers.engine_interfaces.CPUTypes;
import com.maxeler.maxcompiler.v2.managers.engine_interfaces.EngineInterface;
// import com.maxeler.maxcompiler.v2.managers.engine_interfaces.InterfaceParam;

import maxdeep.kernels.*;
import maxdeep.managers.NetworkManager;
import maxdeep.graph.NeuralNetworkGraph;

import java.util.*;
import java.util.logging.*;

public class NetworkOptimiser {

  private final static Logger LOGGER =
    Logger.getLogger(NetworkOptimiser.class.getName());

  private static final CPUTypes DEFAULT_CPU_INP_TYPE = CPUTypes.FLOAT;
  private static final CPUTypes DEFAULT_CPU_OUT_TYPE = CPUTypes.FLOAT;
  private static final String DEFAULT_CPU_INP_NAME = "cpu_inp";
  private static final String DEFAULT_CPU_OUT_NAME = "cpu_out";

  private Map<String, KernelBlock> kernelBlocks;

  public NetworkOptimiser() {
    LOGGER.setLevel(Level.INFO);
  }

  public void optimise(NetworkManager manager, NeuralNetworkGraph graph) {
    kernelBlocks = new HashMap<String, KernelBlock>();
    EngineInterface ei = new EngineInterface();

    // setup input stream from CPU
    int inputStreamSize = graph.getInputLayerSize();
    LOGGER.info("Input stream size: " + inputStreamSize);

    DFELink CPUInp = manager.addStreamFromCPU(DEFAULT_CPU_INP_NAME);
    ei.setStream(DEFAULT_CPU_INP_NAME, DEFAULT_CPU_INP_TYPE,
        inputStreamSize * DEFAULT_CPU_INP_TYPE.sizeInBytes());
    
    // create kernel in sequence
    int idx;
    int numOfLayers = graph.getNumOfLayers();
    for (idx = 1; idx < numOfLayers; idx ++) {
      // skip the first index, which is the input layer
      String name = null;
      KernelBlock knl = null;

      if (graph.isConvLayer(idx)) {
        int H = graph.getInputHeight(idx);
        int W = graph.getInputWidth(idx);
        int C = graph.getInputChannel(idx);
        int F = graph.getOutputChannel(idx);
        int K = graph.getKernelSize(idx);

        name = graph.getLayerName(idx);
        LOGGER.info("Creating CONV layer: " + name);
        LOGGER.info("H = " + Integer.toString(H));
        LOGGER.info("W = " + Integer.toString(W));
        LOGGER.info("C = " + Integer.toString(C));
        LOGGER.info("F = " + Integer.toString(F));
        LOGGER.info("K = " + Integer.toString(K));

        knl = manager.addKernel(
            new ConvLayerKernel(
              manager.makeKernelParameters(graph.getLayerName(idx)),
              H, W, F, C, K));

        // add a stream for the weight input
        String wgtName = name + "_wgt";
        DFELink wgtFromCPU = manager.addStreamFromCPU(wgtName);
        knl.getInput("wgt") <== wgtFromCPU;

      } else if (graph.isFCLayer(idx)) {
        int H = graph.getInputHeight(idx);
        int W = graph.getInputWidth(idx);
        int C = graph.getInputChannel(idx);
        int numOutput = graph.getOutputChannel(idx);

        name = graph.getLayerName(idx);

        LOGGER.info("Creating FC layer: " + name);
        LOGGER.info("H = " + Integer.toString(H));
        LOGGER.info("W = " + Integer.toString(W));
        LOGGER.info("C = " + Integer.toString(C));
        LOGGER.info("M = " + Integer.toString(H * W * C));
        LOGGER.info("numOutput = " + Integer.toString(numOutput));

        knl = manager.addKernel(
            new FCLayerKernel(
              manager.makeKernelParameters(name),
              H * W * C, numOutput));

        String wgtName = name + "_wgt";
        DFELink wgtFromCPU = manager.addStreamFromCPU(wgtName);
        knl.getInput("wgt") <== wgtFromCPU;

      } else if (graph.isPoolingLayer(idx)) {
        int H = graph.getInputHeight(idx);
        int W = graph.getInputWidth(idx);
        int C = graph.getInputChannel(idx);
        int S = graph.getStride(idx);
        int K = graph.getKernelSize(idx);

        name = graph.getLayerName(idx);

        LOGGER.info("Creating Pooling layer: " + name);
        LOGGER.info("H = " + Integer.toString(H));
        LOGGER.info("W = " + Integer.toString(W));
        LOGGER.info("C = " + Integer.toString(C));
        LOGGER.info("K = " + Integer.toString(K));
        LOGGER.info("S = " + Integer.toString(S));

        knl = manager.addKernel(
            new PoolingLayerKernel(
              manager.makeKernelParameters(name),
              H, W, C, K, S));

      } else if (graph.isReLULayer(idx)) {
        name = graph.getLayerName(idx);
        LOGGER.info("Creating ReLU layer: " + name);
        knl = manager.addKernel(
            new ReLULayerKernel(manager.makeKernelParameters(name)));
      } else {
        throw new UnsupportedOperationException("Layer type not supported");
      }

      kernelBlocks.put(name, knl);
    }
    
    // connect kernels to each other by using top and bottom
    // the connection is established through blob
    for (idx = 1; idx < numOfLayers; idx ++) {
      // connect its top to the bottom blobs it belongs to
      String btm = graph.getLayerBtmName(idx);
      String top = graph.getLayerTopName(idx);
      KernelBlock btmKnl = kernelBlocks.get(graph.getLayerName(idx));
      int topIdx = graph.getBlobTopOfLayer(btm);

      LOGGER.info("[" + btm + "] -> [" + top + "]");
      
      if (graph.isInputLayer(topIdx)) {
        btmKnl.getInput("inp") <== CPUInp;
      }
      else {
        KernelBlock topKnl = kernelBlocks.get(graph.getLayerName(topIdx)); 
        btmKnl.getInput("inp") <== topKnl.getOutput("opt");
      }

      if (idx == numOfLayers - 1) {
        DFELink CPUOut = manager.addStreamToCPU(DEFAULT_CPU_OUT_NAME);

        CPUOut <== btmKnl.getOutput("opt");
        ei.setStream(DEFAULT_CPU_OUT_NAME, DEFAULT_CPU_OUT_TYPE,
          graph.getLayerOutputSize(idx) * DEFAULT_CPU_OUT_TYPE.sizeInBytes());
      }
    }
    
    
    manager.createSLiCinterface(ei);
  }
}
