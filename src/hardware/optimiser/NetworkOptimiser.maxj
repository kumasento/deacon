package maxdeep.optimiser;

// import com.maxeler.maxcompiler.v2.build.EngineParameters;
import com.maxeler.maxcompiler.v2.managers.custom.DFELink;
import com.maxeler.maxcompiler.v2.managers.custom.blocks.KernelBlock;
import com.maxeler.maxcompiler.v2.managers.engine_interfaces.CPUTypes;
import com.maxeler.maxcompiler.v2.managers.custom.ManagerClock;
import com.maxeler.maxcompiler.v2.managers.engine_interfaces.EngineInterface;
import com.maxeler.maxcompiler.v2.managers.engine_interfaces.InterfaceParam;
import com.maxeler.maxcompiler.v2.kernelcompiler.types.base.DFEType;
import com.maxeler.maxcompiler.v2.kernelcompiler.types.base.DFETypeFactory;
import com.maxeler.maxcompiler.v2.kernelcompiler.types.base.DFEFix.SignMode;

import maxdeep.kernels.*;
import maxdeep.managers.NetworkManager;
import maxdeep.graph.NeuralNetworkGraph;

import java.util.*;
import java.util.logging.*;

public class NetworkOptimiser {

  private final static Logger LOGGER =
    Logger.getLogger(NetworkOptimiser.class.getName());

  private static final int DEFAULT_FREQ = 100;

  private ManagerClock defaultClockSource;

  private CPUTypes defaultCPUType    = CPUTypes.FLOAT;
  private CPUTypes defaultCPUInpType = CPUTypes.FLOAT;
  private CPUTypes defaultCPUOutType = CPUTypes.FLOAT;
  private DFEType defaultDFEDataType = DFETypeFactory.dfeFloat(8, 24);
  private DFEType defaultCPUDFEType  = DFETypeFactory.dfeFloat(8, 24);

  private static final String DEFAULT_CPU_INP_NAME = "cpu_inp";
  private static final String DEFAULT_CPU_OUT_NAME = "cpu_out";

  private static final String DEFAULT_LAYER_OUT_SUFFIX = "_out";
  private static final String DEFAULT_LAYER_INP_SUFFIX = "_inp";

  private static final int MAX_MEM_SIZE = 32768;

  private Map<String, KernelBlock> kernelBlocks;
  
  private EngineInterface ei;

  // size in bytes
  private static final int STREAM_ALIGN_SIZE = 16;

  public NetworkOptimiser() {
    LOGGER.setLevel(Level.INFO);
  }

  public void optimise(NetworkManager manager, NeuralNetworkGraph graph) {
    kernelBlocks = new HashMap<String, KernelBlock>();
    ei = new EngineInterface();

    manager.config.setDefaultStreamClockFrequency(DEFAULT_FREQ);
    defaultClockSource = manager.generateStreamClock("default_clock",
        DEFAULT_FREQ);

    // Setup input stream from CPU {{{
    
    int inputStreamSize = graph.getInputLayerSize();
    LOGGER.info("Input stream size: " + inputStreamSize);

    DFELink CPUInp = manager.addStreamFromCPU(DEFAULT_CPU_INP_NAME);
    ei.setStream(DEFAULT_CPU_INP_NAME, defaultCPUInpType,
        inputStreamSize * defaultCPUInpType.sizeInBytes());
    
    // }}}
    // Create kernels {{{

    int idx;
    int numOfLayers = graph.getNumOfLayers();
    for (idx = 1; idx < numOfLayers; idx ++) {
      // skip the first index, which is the input layer
      String name = null;
      KernelBlock knl = null;

      // Conv Layer {{{
      if (graph.isConvLayer(idx))
        createConvLayerKernels(idx, manager, graph);
      // }}}
      // FC Layer {{{
      else if (graph.isFCLayer(idx))
        createFCLayerKernels(idx, manager, graph);
      // }}}
      // Pooling Layer {{{
      else if (graph.isPoolingLayer(idx)) {
        int H = graph.getInputHeight(idx);
        int W = graph.getInputWidth(idx);
        int C = graph.getInputChannel(idx);
        int S = graph.getStride(idx);
        int P = graph.getPadding(idx);
        int K = graph.getKernelSize(idx);

        name = graph.getLayerName(idx);
        DFEType dfeType = getDFEType(graph.getDataType(idx));

        LOGGER.info("Creating Pooling layer: " + name);
        LOGGER.info("H = " + Integer.toString(H));
        LOGGER.info("W = " + Integer.toString(W));
        LOGGER.info("C = " + Integer.toString(C));
        LOGGER.info("K = " + Integer.toString(K));
        LOGGER.info("S = " + Integer.toString(S));

        knl = manager.addKernel(
            new PoolingLayerKernel(
              manager.makeKernelParameters(name),
              H, W, C, K, P, S, dfeType));

        ei.setTicks(name, C * H * W);
      }
      // }}}
      // ReLU Layer {{{
      else if (graph.isReLULayer(idx)) {
        int H = graph.getInputHeight(idx);
        int W = graph.getInputWidth(idx);
        int C = graph.getInputChannel(idx);

        name = graph.getLayerName(idx);
        DFEType dfeType = getDFEType(graph.getDataType(idx));
        LOGGER.info("Creating ReLU layer: " + name);
        knl = manager.addKernel(
            new ReLULayerKernel(manager.makeKernelParameters(name),
              dfeType));
        ei.setTicks(name, C * H * W);
      }
      // }}}
      else {
        throw new UnsupportedOperationException("Layer type not supported");
      }

      kernelBlocks.put(name, knl);
    }

    // }}}
    // Connect KernelBlocks {{{

    // connect kernels to each other by using top and bottom
    // the connection is established through blob
    for (idx = 1; idx < numOfLayers; idx ++) {
      // connect its top to the bottom blobs it belongs to
      // the bottom blob is the top blob from the previous layer
      String btm = graph.getLayerBtmName(idx);
      String top = graph.getLayerTopName(idx);

      // get kernel with input and output stream for the bottom
      KernelBlock btmInpKnl = getLayerInpKernel(idx, graph);
      KernelBlock btmOutKnl = getLayerOutKernel(idx, graph);

      // topIdx is the idx for the layer which has bottom blob [btm]
      int topIdx = graph.getBlobTopOfLayer(btm);

      LOGGER.info("[" + btm + "] -> [" + top + "]");
      
      // the bottom of this layer will take input from 
      if (graph.isInputLayer(topIdx)) {
        KernelBlock castKnl = createTypeCastKernel(
            "cpu_inp_cast",
            defaultCPUDFEType,
            getDFEType(graph.getDataType(idx)),
            inputStreamSize,
            manager);
        castKnl.getInput(TypeCastKernel.INP_NAME) 
          <== CPUInp;
        btmInpKnl.getInput("inp")
          <== castKnl.getOutput(TypeCastKernel.OUT_NAME);
      }
      else {
        KernelBlock topKnl = getLayerOutKernel(topIdx, graph);
        btmInpKnl.getInput("inp") <== topKnl.getOutput("opt");
      }

      if (idx == numOfLayers - 1) {
        DFELink CPUOut = manager.addStreamToCPU(DEFAULT_CPU_OUT_NAME);
        int outputSize = graph.getLayerOutputSize(idx) * defaultCPUOutType.sizeInBytes();
        ei.setStream(DEFAULT_CPU_OUT_NAME, defaultCPUOutType, alignStreamSize(outputSize));

        KernelBlock castKnl = createTypeCastKernel(
            "cpu_out_cast",
            getDFEType(graph.getDataType(idx)),
            defaultCPUDFEType,
            graph.getLayerOutputSize(idx),
            manager);

        castKnl.getInput(TypeCastKernel.INP_NAME)
          <== btmOutKnl.getOutput("opt");
        CPUOut <== 
          castKnl.getOutput(TypeCastKernel.OUT_NAME);

        LOGGER.info("output size: " + Integer.toString(outputSize));
      }
    }
    // }}}
    
    manager.createSLiCinterface(ei);
  }

  private KernelBlock getLayerInpKernel(int layerIdx, NeuralNetworkGraph graph) {
    return (
      (graph.isConvLayer(layerIdx) || graph.isFCLayer(layerIdx))
      ? kernelBlocks.get(graph.getLayerName(layerIdx) + DEFAULT_LAYER_INP_SUFFIX)
      : kernelBlocks.get(graph.getLayerName(layerIdx))
    );
  }

  private KernelBlock getLayerOutKernel(int layerIdx, NeuralNetworkGraph graph) {
    return (
      (graph.isConvLayer(layerIdx) || graph.isFCLayer(layerIdx))
      ? kernelBlocks.get(graph.getLayerName(layerIdx) + DEFAULT_LAYER_OUT_SUFFIX)
      : kernelBlocks.get(graph.getLayerName(layerIdx))
    );
  }

  /**
   * Create kernels for a given convolution layer.
   *
   * The layer is given by a layer index, information about this layer is from 
   * graph and manager. There is no output for this layer, and the kernelBlocks
   * dictionary will be updated with kernels in the convolution layer.
   *
   * @param layerIdx the index for this convolution layer
   * @param manager the DFE manager to attach the convolution layer kernels
   * @param graph the high level description of the neural network
   */
  private void createConvLayerKernels(int layerIdx, NetworkManager manager, NeuralNetworkGraph graph) {
    // Fetch layer parameters {{{
    int H   = graph.getInputHeight(layerIdx);
    int W   = graph.getInputWidth(layerIdx);
    int C   = graph.getInputChannel(layerIdx);
    int F   = graph.getOutputChannel(layerIdx);
    int K   = graph.getKernelSize(layerIdx);
    int P   = graph.getPadding(layerIdx);
    int S   = graph.getStride(layerIdx);
    int oH  = (H + 2 * P - K) / S + 1;
    int oW  = (W + 2 * P - K) / S + 1;
    int P_F = 2;
    int P_F_MP = 1;

    String name = graph.getLayerName(layerIdx);
    DFEType dfeType = getDFEType(graph.getDataType(layerIdx));

    LOGGER.info("Creating CONV layer: " + name);
    LOGGER.info("H = " + Integer.toString(H));
    LOGGER.info("W = " + Integer.toString(W));
    LOGGER.info("C = " + Integer.toString(C));
    LOGGER.info("F = " + Integer.toString(F));
    LOGGER.info("K = " + Integer.toString(K));
    LOGGER.info("P = " + Integer.toString(P));
    LOGGER.info("S = " + Integer.toString(S));
    LOGGER.info("P_F = " + Integer.toString(P_F));

    // validators
    if (F < P_F || F % P_F != 0)
      throw new IllegalArgumentException("S * P_F should be divided by W");
    // if (K * K * C * F >= MAX_MEM_SIZE) 
    //   throw new IllegalArgumentException("Weight FMem size (" + Integer.toString(K * K * C * F) + ") should not be larger than "
    //       + Integer.toString(MAX_MEM_SIZE));
    // }}}
    // Streams {{{
    DFELink wgtsCPU = createDFELinkFromCPU(name + "_wgts", K * K * C * F,
        manager);
    KernelBlock castKnl = createTypeCastKernel(
        name + "_wgts_cast",
        defaultCPUDFEType,
        dfeType,
        K * K * C * F,
        manager);
    castKnl.getInput(TypeCastKernel.INP_NAME)
      <== wgtsCPU;
    DFELink wgts = castKnl.getOutput(TypeCastKernel.OUT_NAME);
    // }}}
    // Input kernel block {{{

    String inp_name = name + DEFAULT_LAYER_INP_SUFFIX;
    KernelBlock inp_knl = manager.addKernel(
        new ConvLayerKernel(
          manager.makeKernelParameters(inp_name),
          H, W, F, C, K, P, S, P_F, false,
          dfeType));

    // }}}
    // Accumulate kernel block {{{

    String acc_name = name + "_acc";
    KernelBlock acc_knl = manager.addKernel(
        new ConvAccumKernel(
          manager.makeKernelParameters(acc_name),
          H, W, F, C, K, P, S, P_F,
          dfeType));

    // }}}
    // Convolve kernel block {{{

    ManagerClock convKnlClk = manager.generateStreamClock(
        name + "_clk", DEFAULT_FREQ * P_F_MP);
    for (int f = 0; f < P_F; f += P_F_MP) {
      int fIdx = f / P_F_MP;
      String suffix = "_" + Integer.toString(fIdx);
      String conv_name = name + "_conv" + suffix;

      KernelBlock conv_knl = manager.addKernel(
          new ConvolveKernel(
            manager.makeKernelParameters(conv_name), K, P_F_MP,
            dfeType));
      conv_knl.setClock(convKnlClk);

      // update kernelBlocks
      kernelBlocks.put(conv_name, conv_knl);

      for (int m = 0; m < P_F_MP; m ++) {
        int sIdx = f + m;
        String streamSuffix = "_" + Integer.toString(sIdx);
        String mSuffix = "_" + Integer.toString(m);
        // receive input feature map and weights from the input kernel
        conv_knl.getInput(ConvolveKernel.INP_NAME + mSuffix)
          <== inp_knl.getOutput(ConvLayerKernel.CONV_KNL_INP_NAME + streamSuffix);
        conv_knl.getInput(ConvolveKernel.WGT_NAME + mSuffix)
          <== inp_knl.getOutput(ConvLayerKernel.CONV_KNL_WGT_NAME + streamSuffix);

        // send convolved result to the accumulate kernel
        acc_knl.getInput(ConvAccumKernel.CONV_KNL_OUT_NAME + streamSuffix)
          <== conv_knl.getOutput(ConvolveKernel.OUT_NAME + mSuffix);
      }

      ei.setTicks(conv_name, H * W * F * C / P_F * P_F_MP);
    }

    // }}}
    // Output kernel block {{{

    String out_name = name + DEFAULT_LAYER_OUT_SUFFIX;
    KernelBlock out_knl = manager.addKernel(
        new ConvOutputKernel(
          manager.makeKernelParameters(out_name),
          oH * oW, F, P_F, dfeType));

    // }}}
    // Connections {{{

    inp_knl.getInput(ConvLayerKernel.WGTS_NAME)
      <== wgts;
    out_knl.getInput(ConvOutputKernel.INP_NAME)
      <== acc_knl.getOutput(ConvAccumKernel.OUT_NAME);

    // }}}
    // Setup ticks {{{

    ei.setTicks(inp_name, H * W * F * C / P_F);
    ei.setTicks(acc_name, H * W * F * C / P_F);
    ei.setTicks(out_name, oH * oW * F);

    // }}}
    // Update kernelBlocks {{{

    kernelBlocks.put(inp_name, inp_knl);
    kernelBlocks.put(acc_name, acc_knl);
    kernelBlocks.put(out_name, out_knl);

    // }}}
  }

  /**
   * Create kernels for a given FC layer.
   *
   * @param layerIdx the index for this FC layer
   * @param manager the DFE manager to attach the FC layer kernels
   * @param graph the high level description of the neural network
   */
  private void createFCLayerKernels(int layerIdx, NetworkManager manager, NeuralNetworkGraph graph) {
    // Fetch layer parameters {{{
    int H   = graph.getInputHeight(layerIdx);
    int W   = graph.getInputWidth(layerIdx);
    int C   = graph.getInputChannel(layerIdx);
    int N   = H * W * C;
    int M   = graph.getOutputChannel(layerIdx);
    int P_V = 1;

    String name = graph.getLayerName(layerIdx);
    DFEType dfeType = getDFEType(graph.getDataType(layerIdx));

    LOGGER.info("Creating FC layer: " + name);
    LOGGER.info("H = " + Integer.toString(H));
    LOGGER.info("W = " + Integer.toString(W));
    LOGGER.info("C = " + Integer.toString(C));
    LOGGER.info("M = " + Integer.toString(M));
    LOGGER.info("N = " + Integer.toString(N));
    LOGGER.info("P_V = " + Integer.toString(P_V));

    if (N >= MAX_MEM_SIZE) 
      throw new IllegalArgumentException(
          "number of columns (" + Integer.toString(N) + ") " +
          "should not be larger than " +
          "MAX_MEM_SIZE (" + Integer.toString(MAX_MEM_SIZE) + ")");

    // }}}
    // Streams {{{

    DFELink wgtsCPU = createDFELinkFromCPU(name + "_wgts", M * N, manager);
    DFELink biasCPU = createDFELinkFromCPU(name + "_bias", M, manager);

    KernelBlock castWgtsKnl = createTypeCastKernel(
        name + "_wgts_cast",
        defaultCPUDFEType,
        dfeType,
        M * N,
        manager);
    castWgtsKnl.getInput(TypeCastKernel.INP_NAME)
      <== wgtsCPU;
    DFELink wgts = castWgtsKnl.getOutput(TypeCastKernel.OUT_NAME);

    KernelBlock castBiasKnl = createTypeCastKernel(
        name + "_bias_cast",
        defaultCPUDFEType,
        dfeType,
        M,
        manager);
    castBiasKnl.getInput(TypeCastKernel.INP_NAME)
      <== biasCPU;
    DFELink bias = castBiasKnl.getOutput(TypeCastKernel.OUT_NAME);
    // }}}
    // Input kernel block {{{

    String inp_name = name + "_inp";
    KernelBlock inp_knl = manager.addKernel(
        new FCLayerInpKernel(
          manager.makeKernelParameters(inp_name), M, N, P_V,
          dfeType));

    // }}}
    // Output kernel block {{{

    String out_name = name + "_out";
    KernelBlock out_knl = manager.addKernel(
        new FCLayerOutKernel(
          manager.makeKernelParameters(out_name), P_V,
          dfeType));

    // }}}
    // Dot product kernels {{{
    for (int p = 0; p < P_V; p ++) {
      String suffix = "_" + Integer.toString(p);
      String dot_name = name + "_dot" + suffix;

      KernelBlock dot_knl = manager.addKernel(
          new FCLayerDotProductKernel(
            manager.makeKernelParameters(dot_name), M / P_V, N,
            dfeType));

      // loop offset
      InterfaceParam L = ei.getAutoLoopOffset(dot_name, "fcLoopLength");
      ei.ignoreAutoLoopOffset(dot_name, "fcLoopLength");

      dot_knl.getInput(FCLayerDotProductKernel.INP_NAME)
        <== inp_knl.getOutput(FCLayerInpKernel.INP_NAME + suffix);
      dot_knl.getInput(FCLayerDotProductKernel.WGT_NAME)
        <== inp_knl.getOutput(FCLayerInpKernel.WGT_NAME + suffix);
      dot_knl.getInput(FCLayerDotProductKernel.BIAS_NAME)
        <== inp_knl.getOutput(FCLayerInpKernel.BIAS_NAME + suffix);
      out_knl.getInput(FCLayerOutKernel.INP_NAME + suffix)
        <== dot_knl.getOutput(FCLayerDotProductKernel.OUT_NAME);

      kernelBlocks.put(dot_name, dot_knl);
      ei.setTicks(dot_name, M / P_V * N * L);
    }
    // }}}
    // Connect Kernels {{{
    inp_knl.getInput(FCLayerInpKernel.WGT_NAME) <== wgts;
    inp_knl.getInput(FCLayerInpKernel.BIAS_NAME) <== bias;
    // }}}
    // Update kernelBlocks {{{
    kernelBlocks.put(inp_name, inp_knl);
    kernelBlocks.put(out_name, out_knl);
    // }}}
    // Ticks {{{
    ei.setTicks(inp_name, M * N / P_V);
    ei.setTicks(out_name, M / P_V);
    // }}}
  }

  /**
   * Align the stream size.
   *
   * Streams in MaxJ should be aligned to a specific size (typically 16 bytes),
   * this function will align the original stream size to its nearest upper 
   * aligned bound.
   *
   * @param originSize the original stream size
   * @return the aligned stream size
   */
  private int alignStreamSize(int originSize) {
    return (
        (originSize % STREAM_ALIGN_SIZE != 0)
        ? ((int) (Math.floor((double) originSize / STREAM_ALIGN_SIZE) + 1)
          * STREAM_ALIGN_SIZE) 
        : originSize);
  }

  /**
   * Create a DFELink from CPU with stream name and size.
   *
   * @param name name of the DFELink
   * @param size the size of the stream
   * @param manager the DFE manager
   * @return the generated DFELink
   */
  private DFELink createDFELinkFromCPU(String name, int size, NetworkManager manager) {
    DFELink link = manager.addStreamFromCPU(name);
    int sizeInBytes = size * defaultCPUType.sizeInBytes();
    ei.setStream(name, defaultCPUType, sizeInBytes);
    return link;
  }
  
  private KernelBlock createTypeCastKernel(
      String name, 
      DFEType inpType,
      DFEType outType,
      long ticks,
      NetworkManager manager
  ) {
    KernelBlock knl = manager.addKernel(
        new TypeCastKernel(
          manager.makeKernelParameters(name),
          inpType, outType));
    ei.setTicks(name, ticks);
    return knl;
  }

  private DFEType getDFEType(NeuralNetworkGraph.DataType type) {
    switch(type) {
      case FLOAT32:
        return DFETypeFactory.dfeFloat(8, 24);
      case FIXED16:
        return DFETypeFactory.dfeFix(8, 8, SignMode.TWOSCOMPLEMENT);
      default:
        throw new IllegalArgumentException("Conversion from DataType" +
            " to DFEType has not been implemented yet");
    }
  }
}
