package maxdeep.optimiser;

// import com.maxeler.maxcompiler.v2.build.EngineParameters;
import com.maxeler.maxcompiler.v2.managers.custom.DFELink;
import com.maxeler.maxcompiler.v2.managers.custom.blocks.KernelBlock;
import com.maxeler.maxcompiler.v2.managers.engine_interfaces.CPUTypes;
import com.maxeler.maxcompiler.v2.managers.engine_interfaces.EngineInterface;
import com.maxeler.maxcompiler.v2.managers.engine_interfaces.InterfaceParam;

import maxdeep.kernels.*;
import maxdeep.managers.NetworkManager;
import maxdeep.graph.NeuralNetworkGraph;

import java.util.*;
import java.util.logging.*;

public class NetworkOptimiser {

  private final static Logger LOGGER =
    Logger.getLogger(NetworkOptimiser.class.getName());

  private static final CPUTypes DEFAULT_CPU_TYPE = CPUTypes.FLOAT;
  private static final CPUTypes DEFAULT_CPU_INP_TYPE = CPUTypes.FLOAT;
  private static final CPUTypes DEFAULT_CPU_OUT_TYPE = CPUTypes.FLOAT;

  private static final String DEFAULT_CPU_INP_NAME = "cpu_inp";
  private static final String DEFAULT_CPU_OUT_NAME = "cpu_out";

  private static final String DEFAULT_LAYER_OUT_SUFFIX = "_out";
  private static final String DEFAULT_LAYER_INP_SUFFIX = "_inp";

  private Map<String, KernelBlock> kernelBlocks;
  
  private EngineInterface ei;

  // size in bytes
  private static final int STREAM_ALIGN_SIZE = 16;

  public NetworkOptimiser() {
    LOGGER.setLevel(Level.INFO);
  }

  public void optimise(NetworkManager manager, NeuralNetworkGraph graph) {
    kernelBlocks = new HashMap<String, KernelBlock>();
    ei = new EngineInterface();

    // Setup input stream from CPU {{{
    
    int inputStreamSize = graph.getInputLayerSize();
    LOGGER.info("Input stream size: " + inputStreamSize);

    DFELink CPUInp = manager.addStreamFromCPU(DEFAULT_CPU_INP_NAME);
    ei.setStream(DEFAULT_CPU_INP_NAME, DEFAULT_CPU_INP_TYPE,
        inputStreamSize * DEFAULT_CPU_INP_TYPE.sizeInBytes());
    
    // }}}
    // Create kernels {{{

    int idx;
    int numOfLayers = graph.getNumOfLayers();
    for (idx = 1; idx < numOfLayers; idx ++) {
      // skip the first index, which is the input layer
      String name = null;
      KernelBlock knl = null;

      // Conv Layer {{{
      if (graph.isConvLayer(idx))
        createConvLayerKernels(idx, manager, graph);
      // }}}
      // FC Layer {{{
      else if (graph.isFCLayer(idx))
        createFCLayerKernels(idx, manager, graph);
      // }}}
      // Pooling Layer {{{
      else if (graph.isPoolingLayer(idx)) {
        int H = graph.getInputHeight(idx);
        int W = graph.getInputWidth(idx);
        int C = graph.getInputChannel(idx);
        int S = graph.getStride(idx);
        int P = graph.getPadding(idx);
        int K = graph.getKernelSize(idx);

        name = graph.getLayerName(idx);

        LOGGER.info("Creating Pooling layer: " + name);
        LOGGER.info("H = " + Integer.toString(H));
        LOGGER.info("W = " + Integer.toString(W));
        LOGGER.info("C = " + Integer.toString(C));
        LOGGER.info("K = " + Integer.toString(K));
        LOGGER.info("S = " + Integer.toString(S));

        knl = manager.addKernel(
            new PoolingLayerKernel(
              manager.makeKernelParameters(name),
              H, W, C, K, P, S));

        ei.setTicks(name, C * H * W);
      }
      // }}}
      // ReLU Layer {{{
      else if (graph.isReLULayer(idx)) {
        int H = graph.getInputHeight(idx);
        int W = graph.getInputWidth(idx);
        int C = graph.getInputChannel(idx);

        name = graph.getLayerName(idx);
        LOGGER.info("Creating ReLU layer: " + name);
        knl = manager.addKernel(
            new ReLULayerKernel(manager.makeKernelParameters(name)));
        ei.setTicks(name, C * H * W);
      }
      // }}}
      else {
        throw new UnsupportedOperationException("Layer type not supported");
      }

      kernelBlocks.put(name, knl);
    }

    // }}}
    // Connect KernelBlocks {{{

    // connect kernels to each other by using top and bottom
    // the connection is established through blob
    for (idx = 1; idx < numOfLayers; idx ++) {
      // connect its top to the bottom blobs it belongs to
      // the bottom blob is the top blob from the previous layer
      String btm = graph.getLayerBtmName(idx);
      String top = graph.getLayerTopName(idx);

      // get kernel with input and output stream for the bottom
      KernelBlock btmInpKnl = getLayerInpKernel(idx, graph);
      KernelBlock btmOutKnl = getLayerOutKernel(idx, graph);

      // topIdx is the idx for the layer which has bottom blob [btm]
      int topIdx = graph.getBlobTopOfLayer(btm);

      LOGGER.info("[" + btm + "] -> [" + top + "]");
      
      // the bottom of this layer will take input from 
      if (graph.isInputLayer(topIdx)) {
        btmInpKnl.getInput("inp") <== CPUInp;
      }
      else {
        KernelBlock topKnl = getLayerOutKernel(topIdx, graph);
        btmInpKnl.getInput("inp") <== topKnl.getOutput("opt");
      }

      if (idx == numOfLayers - 1) {
        DFELink CPUOut = manager.addStreamToCPU(DEFAULT_CPU_OUT_NAME);

        CPUOut <== btmOutKnl.getOutput("opt");
        int outputSize = graph.getLayerOutputSize(idx) * DEFAULT_CPU_OUT_TYPE.sizeInBytes();
        ei.setStream(DEFAULT_CPU_OUT_NAME, DEFAULT_CPU_OUT_TYPE, alignStreamSize(outputSize));
        LOGGER.info("output size: " + Integer.toString(outputSize));
      }
    }
    // }}}
    
    manager.createSLiCinterface(ei);
  }

  private KernelBlock getLayerInpKernel(int layerIdx, NeuralNetworkGraph graph) {
    return (
      (graph.isConvLayer(layerIdx) || graph.isFCLayer(layerIdx))
      ? kernelBlocks.get(graph.getLayerName(layerIdx) + DEFAULT_LAYER_INP_SUFFIX)
      : kernelBlocks.get(graph.getLayerName(layerIdx))
    );
  }

  private KernelBlock getLayerOutKernel(int layerIdx, NeuralNetworkGraph graph) {
    return (
      (graph.isConvLayer(layerIdx) || graph.isFCLayer(layerIdx))
      ? kernelBlocks.get(graph.getLayerName(layerIdx) + DEFAULT_LAYER_OUT_SUFFIX)
      : kernelBlocks.get(graph.getLayerName(layerIdx))
    );
  }

  /**
   * Create kernels for a given convolution layer.
   *
   * The layer is given by a layer index, information about this layer is from 
   * graph and manager. There is no output for this layer, and the kernelBlocks
   * dictionary will be updated with kernels in the convolution layer.
   *
   * @param layerIdx the index for this convolution layer
   * @param manager the DFE manager to attach the convolution layer kernels
   * @param graph the high level description of the neural network
   */
  private void createConvLayerKernels(int layerIdx, NetworkManager manager, NeuralNetworkGraph graph) {
    // Fetch layer parameters {{{
    int H   = graph.getInputHeight(layerIdx);
    int W   = graph.getInputWidth(layerIdx);
    int C   = graph.getInputChannel(layerIdx);
    int F   = graph.getOutputChannel(layerIdx);
    int K   = graph.getKernelSize(layerIdx);
    int P   = graph.getPadding(layerIdx);
    int S   = graph.getStride(layerIdx);
    int oH  = (H + 2 * P - K) / S + 1;
    int oW  = (W + 2 * P - K) / S + 1;
    int P_F = 1;

    String name = graph.getLayerName(layerIdx);

    LOGGER.info("Creating CONV layer: " + name);
    LOGGER.info("H = " + Integer.toString(H));
    LOGGER.info("W = " + Integer.toString(W));
    LOGGER.info("C = " + Integer.toString(C));
    LOGGER.info("F = " + Integer.toString(F));
    LOGGER.info("K = " + Integer.toString(K));
    LOGGER.info("P = " + Integer.toString(P));
    LOGGER.info("S = " + Integer.toString(S));
    LOGGER.info("P_F = " + Integer.toString(P_F));

    if (F < P_F || F % P_F != 0)
      throw new IllegalArgumentException("S * P_F should be divided by W");
    // }}}
    // Input kernel block {{{

    String inp_name = name + DEFAULT_LAYER_INP_SUFFIX;
    KernelBlock inp_knl = manager.addKernel(
        new ConvLayerKernel(
          manager.makeKernelParameters(inp_name),
          H, W, F, C, K, P, S, P_F));

    // }}}
    // Accumulate kernel block {{{

    String acc_name = name + "_acc";
    KernelBlock acc_knl = manager.addKernel(
        new ConvAccumKernel(
          manager.makeKernelParameters(acc_name),
          H, W, F, C, K, P, S, P_F));

    // }}}
    // Convolve kernel block {{{

    for (int f = 0; f < P_F; f ++) {
      String suffix = "_" + Integer.toString(f);
      String conv_name = name + "_conv" + suffix;

      KernelBlock conv_knl = manager.addKernel(
          new ConvolveKernel(
            manager.makeKernelParameters(conv_name), K));

      // update kernelBlocks
      kernelBlocks.put(conv_name, conv_knl);

      // receive input feature map and weights from the input kernel
      conv_knl.getInput(ConvolveKernel.INP_NAME)
        <== inp_knl.getOutput(ConvLayerKernel.CONV_KNL_INP_NAME + suffix);
      conv_knl.getInput(ConvolveKernel.WGT_NAME)
        <== inp_knl.getOutput(ConvLayerKernel.CONV_KNL_WGT_NAME + suffix);

      // send convolved result to the accumulate kernel
      acc_knl.getInput(ConvAccumKernel.CONV_KNL_OUT_NAME + suffix)
        <== conv_knl.getOutput(ConvolveKernel.OUT_NAME);

      ei.setTicks(conv_name, H * W * F * C / P_F);
    }

    // }}}
    // Output kernel block {{{

    String out_name = name + DEFAULT_LAYER_OUT_SUFFIX;
    KernelBlock out_knl = manager.addKernel(
        new ConvOutputKernel(
          manager.makeKernelParameters(out_name),
          oH * oW, F, P_F));

    // }}}
    // Connections {{{

    out_knl.getInput(ConvOutputKernel.INP_NAME)
      <== acc_knl.getOutput(ConvAccumKernel.OUT_NAME);

    // }}}
    // Setup ticks {{{

    ei.setTicks(inp_name, H * W * F * C / P_F);
    ei.setTicks(acc_name, H * W * F * C / P_F);
    ei.setTicks(out_name, oH * oW * F);

    // }}}
    // Update kernelBlocks {{{

    kernelBlocks.put(inp_name, inp_knl);
    kernelBlocks.put(acc_name, acc_knl);
    kernelBlocks.put(out_name, out_knl);

    // }}}
  }

  /**
   * Create kernels for a given FC layer.
   *
   * @param layerIdx the index for this FC layer
   * @param manager the DFE manager to attach the FC layer kernels
   * @param graph the high level description of the neural network
   */
  private void createFCLayerKernels(int layerIdx, NetworkManager manager, NeuralNetworkGraph graph) {
    // Fetch layer parameters {{{
  
    int H   = graph.getInputHeight(layerIdx);
    int W   = graph.getInputWidth(layerIdx);
    int C   = graph.getInputChannel(layerIdx);
    int N   = H * W * C;
    int M   = graph.getOutputChannel(layerIdx);
    int P_V = 1;

    String name = graph.getLayerName(layerIdx);

    LOGGER.info("Creating FC layer: " + name);
    LOGGER.info("H = " + Integer.toString(H));
    LOGGER.info("W = " + Integer.toString(W));
    LOGGER.info("C = " + Integer.toString(C));
    LOGGER.info("M = " + Integer.toString(M));
    LOGGER.info("N = " + Integer.toString(N));
    LOGGER.info("P_V = " + Integer.toString(P_V));

    // }}}
    // Input kernel block {{{

    String inp_name = name + "_inp";
    KernelBlock inp_knl = manager.addKernel(
        new FCLayerInpKernel(
          manager.makeKernelParameters(inp_name), M, N, P_V));

    // }}}
    // Output kernel block {{{

    String out_name = name + "_out";
    KernelBlock out_knl = manager.addKernel(
        new FCLayerOutKernel(
          manager.makeKernelParameters(out_name), P_V));

    // }}}
    // Dot product kernels {{{
    for (int p = 0; p < P_V; p ++) {
      String suffix = "_" + Integer.toString(p);
      String dot_name = name + "_dot" + suffix;

      KernelBlock dot_knl = manager.addKernel(
          new FCLayerDotProductKernel(
            manager.makeKernelParameters(dot_name), M / P_V, N));

      // loop offset
      InterfaceParam L = ei.getAutoLoopOffset(dot_name, "fcLoopLength");
      ei.ignoreAutoLoopOffset(dot_name, "fcLoopLength");

      dot_knl.getInput(FCLayerDotProductKernel.INP_NAME)
        <== inp_knl.getOutput(FCLayerInpKernel.INP_NAME + suffix);
      dot_knl.getInput(FCLayerDotProductKernel.WGT_NAME)
        <== inp_knl.getOutput(FCLayerInpKernel.WGT_NAME + suffix);
      dot_knl.getInput(FCLayerDotProductKernel.BIAS_NAME)
        <== inp_knl.getOutput(FCLayerInpKernel.BIAS_NAME + suffix);
      out_knl.getInput(FCLayerOutKernel.INP_NAME + suffix)
        <== dot_knl.getOutput(FCLayerDotProductKernel.OUT_NAME);

      kernelBlocks.put(dot_name, dot_knl);
      ei.setTicks(dot_name, M / P_V * N * L);
    }
    // }}}
    // Update kernelBlocks {{{
    kernelBlocks.put(inp_name, inp_knl);
    kernelBlocks.put(out_name, out_knl);
    // }}}
    // Ticks {{{
    ei.setTicks(inp_name, M * N / P_V);
    ei.setTicks(out_name, M / P_V);
    // }}}
  }

  /**
   * Align the stream size.
   *
   * Streams in MaxJ should be aligned to a specific size (typically 16 bytes),
   * this function will align the original stream size to its nearest upper 
   * aligned bound.
   *
   * @param originSize the original stream size
   * @return the aligned stream size
   */
  private int alignStreamSize(int originSize) {
    return (
        (originSize % STREAM_ALIGN_SIZE != 0)
        ? ((int) (Math.floor((double) originSize / STREAM_ALIGN_SIZE) + 1)
          * STREAM_ALIGN_SIZE) 
        : originSize);
  }

  /**
   * Create a DFELink from CPU with stream name and size.
   *
   * @param name name of the DFELink
   * @param sizeInBytes the size of the stream
   * @param manager the DFE manager
   * @return the generated DFELink
   */
  private DFELink createDFELinkFromCPU(String name, int sizeInBytes, NetworkManager manager) {
    DFELink link = manager.addStreamFromCPU(name);
    ei.setStream(name, DEFAULT_CPU_TYPE, alignStreamSize(sizeInBytes));
    return link;
  }
}
